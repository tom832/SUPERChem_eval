# Analysis Scripts

This directory contains a collection of Python scripts for analyzing and visualizing the results of Large Language Model (LLM) evaluations. These scripts process the data generated by the evaluation workflows to produce quantitative metrics and plots.

---

## Script Descriptions

### `calc_pass_withbaseline.py`
This script calculates the pass rates (accuracy) for different models based on their evaluation results. It processes the output files and compares model performance against the ground truth and human baseline data. The primary output is a clean, tabular format of correctness rates, suitable for direct use in reports and papers.

### `draw_correlation_plotly.py`
This script analyzes the relationship between a model's final answer accuracy and its Chain-of-Thought (CoT) reasoning accuracy. It generates a scatter plot using Plotly to visualize this correlation, helping to determine if better reasoning consistently leads to better final answers.

### `multimodal_comprison.py`
This script compares model performance based on different input modalities. It analyzes the impact of providing information as text-only versus providing it in a multimodal format (e.g., including images). The goal is to measure how effectively models utilize multimodal inputs.

### `pass_k_curve.py`
This script analyzes the effect of `pass@k` on model performance. It plots a curve showing how the success rate of a model increases as `k` (the number of generated solutions) increases. This helps visualize the trade-off between computational cost and performance gain from generating multiple responses.

### `break_point_analysis.py`
This script performs a "breakpoint analysis" on the models' Chain-of-Thought (CoT) reasoning. It identifies the first incorrect step in each reasoning chain and aggregates the frequency of these initial errors. This is useful for diagnosing common failure points in a model's reasoning process.

### `set_sample_size.py`
This script helps determine the minimum representative sample size for the dataset. It runs analyses on progressively larger subsets of the data and checks at which point the conclusions (e.g., model rankings) stabilize and remain consistent with the conclusions drawn from the entire dataset. This is crucial for efficient and reliable testing.

### `plot_tag_ckpt_distribution_plotly.py`
This script visualizes the distribution of different "ability tags" across the entire question dataset. It generates a pie chart using Plotly, providing a clear overview of the skills and knowledge areas covered by the benchmark.

### `draw_radar_plotly.py`
This script generates a radar chart to compare the capabilities of several state-of-the-art (SOTA) models across a range of defined abilities. Each axis of the radar chart represents a different ability tag, and the plot shows the relative strengths and weaknesses of each model in a single, comparative visualization.

### `generate_pdf.py`
This script generates PDF files from the question dataset stored in Parquet format. It formats the questions and associated images into a printable PDF document, which can be useful for offline review or human evaluation.
