# CCMEBench: A Benchmark for Deep Chemical Reasoning in Large Language Models

This repository contains the official evaluation framework for **CCMEBench**, an expert-curated, Olympiad-level multimodal benchmark for the rigorous evaluation of deep chemical reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs).

---

## Abstract

Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, ceiling effects, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce **CCMEBench**, a benchmark of 500 expert-curated chemistry problems at Olympiad-level difficulty, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling *Reasoning Path Fidelity* (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%. By combining high difficulty, controlled multimodality, and process-level metrics, CCMEBench provides a rigorous platform for diagnosing and advancing AI chemical reasoning toward expert-level scientific inquiry.

---

## Key Features

-   **Expert-Level Challenge**: 500 Olympiad-level problems curated by domain experts to test deep chemical reasoning and mitigate the ceiling effects seen in other benchmarks.
-   **Process-Level Evaluation**: Introduces **Reasoning Path Fidelity (RPF)**, a metric to assess the alignment of a model's reasoning with expert-authored solution paths, distinguishing genuine understanding from "lucky guesses."
-   **Controlled Multimodality**: Each problem is available in both multimodal (with images) and text-only formats, enabling a rigorous, controlled analysis of a model's ability to integrate visual information.
-   **Fine-Grained Ability Taxonomy**: A systematic categorization of chemical knowledge and reasoning skills supports detailed diagnosis of model strengths and weaknesses across various sub-domains.
-   **Contamination Resistant**: Problems are newly authored or adapted from non-public sources and undergo a rigorous human-in-the-loop curation process to ensure quality and reduce the risk of data leakage from web-scraped training sets.

---

## Repository Structure

This repository provides the tools to run evaluations on the CCMEBench dataset.

```
.
├── eval/               # Scripts for running evaluations and generating model outputs.
├── data/               # Datasets, metadata, human baselines, and raw evaluation results.
├── analysis/           # Scripts for processing results and generating analyses/plots.
└── results/            # Output directory for generated plots and figures.
```

-   **eval/**: Contains the core scripts for running evaluations.
    -   : Runs various model checkpoints to generate answers and tag their abilities.
    -   : Uses a judge model to perform a fine-grained evaluation of a model's reasoning (RPF scoring).

-   **data/**: Stores all necessary data for the evaluations.
    -   `20251015_baseline.csv`: Human performance baseline.
    -   `ability_tags_description.json`: Definitions for all ability tags.
    -   `dataset_split_map.json`: Pre-defined dataset splits based on difficulty.
    -   This folder also serves as the output location for raw data from the  scripts.

-   **analysis/**: Includes Python scripts for post-processing and analyzing the evaluation data. This is where you can generate metrics and visualizations like radar charts, pass@k curves, and breakpoint analyses.

-   **results**/**: This folder is the designated output directory for the visual artifacts (plots, charts, etc.) generated by the scripts in the  directory.

---

## Evaluation Workflow

A typical evaluation workflow follows these steps:

1.  **Configure Evaluation**:
    -   Modify the shell scripts in the  directory (, ) to specify the models you want to test, input files, and other parameters.

2.  **Run Evaluation**:
    -   Execute the scripts from the  directory to generate model answers and perform CoT evaluations.
    -   The raw and evaluated `.jsonl` files will be saved in the  directory.

3.  **Analyze Results**:
    -   Use the Python scripts in the  directory to process the data stored in .
    -   For example, run `calc_pass_withbaseline.py` to get accuracy tables or `draw_radar_plotly.py` to visualize model capabilities.

4.  **View Outputs**:
    -   The plots and figures generated by the analysis scripts will be saved in the  directory.

---

## Citation

If you use CCMEBench or this evaluation framework in your research, please cite our paper:

```bibtex
@article{ccmebench2025,
  title={{CCMEBench: A Benchmark for Deep Chemical Reasoning in Large Language Models}},
  author={Your Name and Co-authors},
  journal={Journal/Conference Name},
  year={2025}
}
```